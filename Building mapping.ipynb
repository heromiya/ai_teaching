{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "200206-Zambia Mapping",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/heromiya/ai_teaching/blob/main/Building%20mapping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3hE7-6o2u-Q"
      },
      "source": [
        "#Revision records\n",
        "2020/02/06\n",
        "* Formatting and translating for Zambia mapping\n",
        "\n",
        "2020/01/28\n",
        "* NIEDミニワークショップ用にラスタライズ機能を追加\n",
        "\n",
        "2020/01/20\n",
        "* 地理空間ビッグデータ利活用『持続可能な観光開発』人材育成プログラム用にフォーク\n",
        "\n",
        "2020/01/07\n",
        "\n",
        "* First version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6_UrmQCXAPP"
      },
      "source": [
        "#0. Installing prerequisite"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hTBQrA_uSmx"
      },
      "source": [
        "# Run this line to mount to your Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEER8t_PGDE0"
      },
      "source": [
        "%%time \n",
        "!apt update\n",
        "#!apt upgrade\n",
        "!apt install gdal-bin python-gdal python3-gdal imagemagick python3-rtree python3-rasterio python3-sklearn python3-numpy python3-descartes python3-geopandas parallel bc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YclCBAmrUT6V"
      },
      "source": [
        "import os, shutil, tempfile, random, glob, cv2, subprocess, errno, datetime, numpy as np, matplotlib.pyplot as plt\n",
        "os.environ['CURL_CA_BUNDLE']='/etc/ssl/certs/ca-certificates.crt'\n",
        "import rasterio, rasterio.mask\n",
        "import tensorflow as tf, keras\n",
        "import pandas as pd, geopandas as gpd, fiona\n",
        "\n",
        "from rasterio.merge import merge\n",
        "from rasterio.plot import show\n",
        "from rasterio.warp import calculate_default_transform, reproject, Resampling\n",
        "from osgeo import ogr, gdal\n",
        "from PIL import Image\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "from PIL import Image\n",
        "from sklearn.metrics import jaccard_similarity_score\n",
        "from keras.models import Model\n",
        "from keras.layers import Conv2D, MaxPooling2D, Input, Conv2DTranspose, Concatenate, BatchNormalization, UpSampling2D\n",
        "from keras.layers import  Dropout, Activation\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import backend as K\n",
        "from keras.utils import plot_model\n",
        "from random import shuffle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y6ePVgRMH-m"
      },
      "source": [
        "Upload image data for training to \"[root]/img/\", annotation vector (polygon) data to \"[root]/ann_vec/\". In Section 1.2 below, the annotation data is extracted for the extent of the image data with coordinates in Geotiff format.\n",
        "\n",
        "Image data for training (Geotiff)- \n",
        "* [root]/img/hogehoge.tif\n",
        "* [root]/img/fugafuga.tif\n",
        "* ...\n",
        "\n",
        "Annotation vector (polygon) for training -    \n",
        "* [root]/ann_vec/higehige.gpkg\n",
        "* [root]/ann_vec/fogafoga.gpkg\n",
        "* ...\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AK7CrVc_YQCk"
      },
      "source": [
        "# Directoly/folder prepration\n",
        "root_folder = 'drive/My Drive/200206-ZambiaMapping/'\n",
        "\n",
        "training_ann_ras = root_folder + 'ann_ras/'\n",
        "training_ann_vec = root_folder + 'ann_vec/'\n",
        "training_img = root_folder + 'img/'\n",
        "test_img = root_folder + 'test_img/'\n",
        "\n",
        "patch_root = tempfile.mkdtemp()\n",
        "#patch_root = root_folder + '/patch/'\n",
        "#os.makedirs(patch_root, exist_ok=True)\n",
        "patch_ann = patch_root + '/patch_ann/'\n",
        "patch_img = patch_root + '/patch_img/'\n",
        "patch_pred = patch_root + '/patch_pred/'\n",
        "\n",
        "pretrained_weights = '/content/drive/My Drive/200206-ZambiaMapping/logs/2020-02-12 01:07:10.756811/weights.2020-02-12 00:11:45.266374.0017-0.0503.hdf5'\n",
        "model_folder = root_folder + '/model/'\n",
        "\n",
        "test_results = root_folder + '/test_results/'\n",
        "timestamp = str(datetime.datetime.now())\n",
        "log_d = root_folder + \"logs/\" + timestamp + '/'\n",
        "os.makedirs(log_d, exist_ok=True) \n",
        "\n",
        "os.makedirs(training_ann_ras, exist_ok=True)\n",
        "os.makedirs(patch_ann, exist_ok=True)\n",
        "os.makedirs(patch_img, exist_ok=True)\n",
        "os.makedirs(model_folder, exist_ok=True)\n",
        "os.makedirs(test_results, exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aP4JaM3Zdfk"
      },
      "source": [
        "#0-2. Hyper parameters\n",
        "- patch_size - image size of training patches. Not recommended to modify.\n",
        "- n_patch - number of training patches in a tile. Greater number will yields higher accuracy, but requires more RAM and time.\n",
        "- n_epoch - Maximum number of epoch (iteration). Greater number leads higher accuracy. > 100 is recommended for finalizing outputs.\n",
        "- n_batch - Number of training data for a single process during the training. Greater number leads faster computing and stable learning process while consuming more physical memory.\n",
        "- learning_rate - If iterations shows unstable changes of accuracy, recommended to set lower values. See [an article](https://towardsdatascience.com/understanding-learning-rates-and-how-it-improves-performance-in-deep-learning-d0d4059c1c10).\n",
        "\n",
        "###Tips in parameter tuning\n",
        "- Long computing time --> Increase batch size.\n",
        "- Out of memory --> decrease batch size and number of patches (n_patch).\n",
        "- Overfitting -- gaps between training accuracy and validation accuracy are increasing along with epochs. --> incease number of patches (n_epoch).\n",
        "- Small improvement of training accuracies by epoch --> Increase learning rate.\n",
        "- Fractuating traning accuraies in a long iterations of epochs --> decrease learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUdh8vYI54SF"
      },
      "source": [
        "n_patch = 100 # Too large number sometimes fails in training. More thab 100 is preferred.\n",
        "batch_size = 256\n",
        "n_epoch = 20  # preferably >= 100 for operational model.\n",
        "learning_rate = 0.0003 # 0.00003 # 0.001 is a default of Adam in Keras.\n",
        "decay_rate = learning_rate / n_epoch\n",
        "\n",
        "depth = 5 # Do not change. > 8 causes crash.\n",
        "patch_size = 64 # multiples of 2^depth\n",
        "\n",
        "\n",
        "b_nb_ratio = 0.5 # ratio of sampled patch tile with buildings : without buildings. Initital training will reuquire oversampling of buildings. In case of no-building images, it samples (1 - b_nb_ratio) * n_patch patches.\n",
        "#resolution = 0.298 # Resampling the image. Original image resolution is about 0.3 m/pixel. Oversampling will be needed for balancing building and nonbuilding segments.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02AXhsvB0rek"
      },
      "source": [
        "#1.Training Dataset Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPvjUO5BMlcS"
      },
      "source": [
        "##1-1. Rasterizing annotation vector data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOWW0u_LguIT"
      },
      "source": [
        "%%script env SHPDIR=\"$training_ann_vec\" MASKDIR=\"$training_ann_ras\" RASDIR=\"$training_img\" bash\n",
        "IFS='\n",
        "'\n",
        "\n",
        "rm -f merged.gpkg\n",
        "for VEC in $(find \"$SHPDIR\" -type f | grep -e \".*\\.shp\" -e \".*\\.sqlite\" -e \".*\\.gpkg\"); do\n",
        "  ogr2ogr -f gpkg -append -update merged.gpkg $VEC -nln merged\n",
        "done\n",
        "\n",
        "rm -rf $MASKDIR\n",
        "mkdir -p $MASKDIR\n",
        "function rasterize(){\n",
        "  IMG=$1\n",
        "  OUTTIF=\"$MASKDIR/$(basename \"$IMG\" | sed 's/\\(\\.[a-zA-Z]\\{3\\}\\)$/-a\\1/g')\"\n",
        "  eval $(gdalinfo \"$IMG\" | grep Corner -A 4 |  sed 's/^\\(.*)\\) (.*)$/\\1/g' | sed 's/ (/=(/g; s/ //g; s/,/ /' | tail -n 4)  \n",
        "  eval $(gdalinfo \"$IMG\" | grep \"Pixel Size\" | sed 's/ //g;s/,/ /g; s/-//g')\n",
        "  gdal_rasterize -burn 1 -of GTiff -a_nodata 0 -ot Byte -tr ${PixelSize[0]} ${PixelSize[1]} -te ${LowerLeft[0]} ${LowerLeft[1]} ${UpperRight[0]} ${UpperRight[1]} merged.gpkg \"$OUTTIF\" \n",
        "} \n",
        "export -f rasterize\n",
        "parallel rasterize {} ::: $(find \"$RASDIR\" -type f | grep -e \".*\\.png$\" -e \".*\\.tif$\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mvj97t_Mg5O"
      },
      "source": [
        "##1-2.Generate patch set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Csm0usVrq1eB"
      },
      "source": [
        "%%script env RASDIR=\"$training_img\" MASKDIR=\"$training_ann_ras\" PATRASDIR=\"$patch_img\" PATMASKDIR=\"$patch_ann\" PATCH_SIZE=\"$patch_size\" N_PATCH=\"$n_patch\" B_NB_RATIO=\"$b_nb_ratio\" bash\n",
        "IFS='\n",
        "'\n",
        "rm -rf \"$PATMASKDIR\" \"$PATRASDIR\"\n",
        "mkdir -p \"$PATMASKDIR\" \"$PATRASDIR\"\n",
        "echo \"$PATMASKDIR\" \"$PATRASDIR\"\n",
        "#for TIF in $(find \"$RASDIR\" -type f | grep -e \".*\\.png$\" -e \".*\\.tif$\"); do\n",
        "function gen_patch() {\n",
        "    TIF=$1\n",
        "  IFS=' '\n",
        "\n",
        "    JSON=$(gdalinfo -json \"$TIF\")\n",
        "    SIZE=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['size'])\" | tr -d [],))\n",
        "    X_SIZE=${SIZE[0]}\n",
        "    Y_SIZE=${SIZE[1]}\n",
        "    MASK_TIF=$MASKDIR/$(basename \"$TIF\" | sed 's/\\(\\.[a-zA-Z]\\{3\\}\\)$/-a\\1/g')\n",
        "\n",
        "    upperRight=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['cornerCoordinates']['upperRight'])\" | tr -d [],))\n",
        "    lowerLeft=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['cornerCoordinates']['lowerLeft'])\" | tr -d [],))\n",
        "    geoTransform=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['geoTransform'])\" | tr -d [],))\n",
        "\n",
        "    PIXEL_SIZE_X=${geoTransform[1]}\n",
        "    PIXEL_SIZE_Y=$(echo ${geoTransform[5]} | tr -d \"-\")\n",
        "    #PIXEL_SIZE_X=$RES\n",
        "    #PIXEL_SIZE_Y=$RES\n",
        "    PATCH_SIZE_GX=$(perl -e \"print $PATCH_SIZE * $PIXEL_SIZE_X\")\n",
        "    PATCH_SIZE_GY=$(perl -e \"print $PATCH_SIZE * $PIXEL_SIZE_Y\")\n",
        "    IMG_EXT=\"${lowerLeft[0]} ${lowerLeft[1]} ${upperRight[0]} ${upperRight[1]}\"\n",
        "    N_BLD=$(ogrinfo -spat $IMG_EXT merged.gpkg -al -so | grep \"Feature Count\" | cut -d \" \" -f 3)\n",
        "\n",
        "    for C_BLD in $(ogrinfo -spat ${lowerLeft[0]} ${lowerLeft[1]} ${upperRight[0]} ${upperRight[1]} -sql \"SELECT ST_Centroid(geom) FROM merged;\" -al merged.gpkg | grep POINT | tr -d 'POINT()' | sed \"s/^ *//g; s/ /,/\" | awk '{printf(\"%s \",$0)}'); do\n",
        "      C_BLD=($(echo $C_BLD | sed 's/,/ /'))\n",
        "\n",
        "      EXT_XMIN=$(perl -e \"print ${C_BLD[0]} - $PATCH_SIZE_GX\")\n",
        "      if [ $(echo \"$EXT_XMIN < ${lowerLeft[0]}\" | bc) -eq 1 ]; then\n",
        "        EXT_XMIN=${lowerLeft[0]}\n",
        "      fi\n",
        "\n",
        "      EXT_YMIN=$(perl -e \"print ${C_BLD[1]} - $PATCH_SIZE_GY\")\n",
        "      if [ $(echo \"$EXT_YMIN < ${lowerLeft[1]}\" | bc) -eq 1 ]; then\n",
        "        EXT_YMIN=${lowerLeft[1]}\n",
        "      fi\n",
        "\n",
        "      # Patches for buildings\n",
        "      i=1\n",
        "      while [ $i -le $(perl -e \"use POSIX qw(floor ceil); print ceil($N_PATCH * $B_NB_RATIO / $N_BLD)\") ]; do\n",
        "          PATCH_XMIN=$(perl -e \"print $EXT_XMIN + rand($PATCH_SIZE_GX)\")\n",
        "          PATCH_YMIN=$(perl -e \"print $EXT_YMIN + rand($PATCH_SIZE_GY)\")\n",
        "\n",
        "          if [ $(echo \"$(perl -e \"print $PATCH_XMIN + $PATCH_SIZE_GX\") > ${upperRight[0]}\" | bc) -eq 1 ]; then\n",
        "            PATCH_XMIN=$(perl -e \"print ${upperRight[0]} - $PATCH_SIZE_GX\")\n",
        "          fi\n",
        "\n",
        "          if [ $(echo \"$(perl -e \"print $PATCH_YMIN + $PATCH_SIZE_GY\") > ${upperRight[1]}\" | bc) -eq 1 ]; then\n",
        "            PATCH_YMIN=$(perl -e \"print ${upperRight[1]} - $PATCH_SIZE_GY\")\n",
        "          fi\n",
        "\n",
        "          PATCH_XMAX=$(perl -e \"print $PATCH_XMIN + $PATCH_SIZE_GX\")\n",
        "          PATCH_YMAX=$(perl -e \"print $PATCH_YMIN + $PATCH_SIZE_GY\")\n",
        "\n",
        "          PATCH_IMG=\"${PATRASDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}-i.tif\"\n",
        "          PATCH_MASK=\"${PATMASKDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}-m.tif\"\n",
        "\n",
        "          #gdalwarp -q -r lanczos -tr $RES $RES -te $PATCH_XMIN $PATCH_YMIN $PATCH_XMAX $PATCH_YMAX \"$TIF\" \"$PATCH_IMG\"\n",
        "          #gdalwarp -q -r lanczos  -tr $RES $RES -te $PATCH_XMIN $PATCH_YMIN $PATCH_XMAX $PATCH_YMAX \"$MASK_TIF\" \"$PATCH_MASK\"\n",
        "          gdal_translate -q -projwin $PATCH_XMIN $PATCH_YMAX $PATCH_XMAX $PATCH_YMIN \"$TIF\" \"$PATCH_IMG\"\n",
        "          gdal_translate -q -projwin $PATCH_XMIN $PATCH_YMAX $PATCH_XMAX $PATCH_YMIN \"$MASK_TIF\" \"$PATCH_MASK\"\n",
        "\n",
        "          # Training data augumentation.\n",
        "          for OPT in -flip -flop \"-rotate 90\" \"-rotate 180\" \"-rotate 270\"; do \n",
        "              convert \"$PATCH_IMG\"  $OPT \"${PATRASDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}$(echo $OPT | sed 's/rotate //g')-i.tif\" >& /dev/null\n",
        "              convert \"$PATCH_MASK\" $OPT \"${PATMASKDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}$(echo $OPT | sed 's/rotate //g')-m.tif\" >& /dev/null\n",
        "          done\n",
        "          # Ending autumentation \n",
        "\n",
        "          i=$(expr $i + 1)\n",
        "      done\n",
        "    done\n",
        "        \n",
        "        # Patches for non-buildings\n",
        "    j=1\n",
        "    while [ $j -le $(perl -e \"use POSIX qw(floor ceil); print ceil($N_PATCH * (1 - $B_NB_RATIO))\") ]; do\n",
        "        PATCH_XMIN=$(perl -e \"print ${lowerLeft[0]} + rand($X_SIZE * $PIXEL_SIZE_X - $PATCH_SIZE_GX)\")\n",
        "        PATCH_YMIN=$(perl -e \"print ${lowerLeft[1]} + rand($Y_SIZE * $PIXEL_SIZE_Y - $PATCH_SIZE_GY)\")\n",
        "        PATCH_XMAX=$(perl -e \"print $PATCH_XMIN + $PATCH_SIZE_GX\")\n",
        "        PATCH_YMAX=$(perl -e \"print $PATCH_YMIN + $PATCH_SIZE_GY\")\n",
        "\n",
        "        PATCH_IMG=\"${PATRASDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}-i.tif\"\n",
        "        PATCH_MASK=\"${PATMASKDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}-m.tif\"\n",
        "\n",
        "        #gdalwarp -q -r lanczos -tr $RES $RES -te $PATCH_XMIN $PATCH_YMIN $PATCH_XMAX $PATCH_YMAX \"$TIF\" \"$PATCH_IMG\"\n",
        "        #gdalwarp -q -r lanczos -tr $RES $RES -te $PATCH_XMIN $PATCH_YMIN $PATCH_XMAX $PATCH_YMAX \"$MASK_TIF\" \"$PATCH_MASK\"\n",
        "        gdal_translate -q -projwin $PATCH_XMIN $PATCH_YMAX $PATCH_XMAX $PATCH_YMIN \"$TIF\" \"$PATCH_IMG\"\n",
        "        gdal_translate -q -projwin $PATCH_XMIN $PATCH_YMAX $PATCH_XMAX $PATCH_YMIN \"$MASK_TIF\" \"$PATCH_MASK\"\n",
        "        # Training data augumentation.\n",
        "        for OPT in -flip -flop \"-rotate 90\" \"-rotate 180\" \"-rotate 270\"; do \n",
        "            convert \"$PATCH_IMG\"  $OPT \"${PATRASDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}$(echo $OPT | sed 's/rotate //g')-i.tif\" >& /dev/null\n",
        "            convert \"$PATCH_MASK\" $OPT \"${PATMASKDIR}/$(basename \"$TIF\")-${PATCH_SIZE}-${PATCH_XMIN}_${PATCH_YMIN}$(echo $OPT | sed 's/rotate //g')-m.tif\" >& /dev/null\n",
        "        done\n",
        "        # Ending autumentation \n",
        "\n",
        "        j=$(expr $j + 1)\n",
        "    done\n",
        "#done\n",
        "}\n",
        "export -f gen_patch\n",
        "parallel gen_patch {} ::: $(find \"$RASDIR\" -type f | grep -e \".*\\.png$\" -e \".*\\.tif$\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys1tw8Bf8dpF"
      },
      "source": [
        "# 2. Applying convolutional nueral network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mh-MgyQM2Qw9"
      },
      "source": [
        "##2-1. Introduction\n",
        "\n",
        "The U-Net model is a simple fully  convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts. \n",
        "\n",
        "*   Contracting Path: we apply a series of conv layers and downsampling layers  (max-pooling) layers to reduce the spatial size \n",
        "*   Expanding Path: we apply a series of upsampling layers to reconstruct the spatial size of the input. \n",
        "\n",
        "The two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1. \n",
        "\n",
        "\n",
        "![alt text](https://blog.playment.io/wp-content/uploads/2018/03/Screen-Shot-2018-09-05-at-3.00.03-PM.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tavQ2-MJ0x9E"
      },
      "source": [
        "## 2-2. Image generators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8tq55g677wp"
      },
      "source": [
        "def image_generator(files, batch_size = batch_size, sz = (patch_size, patch_size)):\n",
        "  #print(files)\n",
        "  while True: \n",
        "    \n",
        "    #extract a random batch \n",
        "    batch = np.random.choice(files, size = batch_size)    \n",
        "    \n",
        "    #variables for collecting batches of inputs and outputs \n",
        "    batch_x = []\n",
        "    batch_y = []\n",
        "    \n",
        "    for f in batch:\n",
        "        \n",
        "        #get the masks. Note that masks are png files \n",
        "        try:\n",
        "          mask = Image.open(patch_ann  + '/' + f.replace('-i','-m'))\n",
        "        except:\n",
        "          continue\n",
        "        mask = np.array(mask.resize(sz))\n",
        "        \n",
        "        batch_y.append(mask)\n",
        "\n",
        "        #preprocess the raw images \n",
        "        raw = Image.open(patch_img + '/' + f)\n",
        "        raw = raw.resize(sz)\n",
        "        raw = np.array(raw)\n",
        "\n",
        "        #check the number of channels because some of the images are RGBA or GRAY\n",
        "        if len(raw.shape) == 2:\n",
        "          raw = np.stack((raw,)*3, axis=-1)\n",
        "        else:\n",
        "          raw = raw[:,:,0:3]\n",
        "\n",
        "        batch_x.append(raw)\n",
        "\n",
        "    #preprocess a batch of images and masks \n",
        "    batch_x = np.array(batch_x)/255.\n",
        "    batch_y = np.array(batch_y)\n",
        "    batch_y = np.expand_dims(batch_y,3)\n",
        "\n",
        "    yield (batch_x, batch_y)      \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvwbmS-YTHEZ"
      },
      "source": [
        "all_files = os.listdir(patch_img)\n",
        "shuffle(all_files)\n",
        "\n",
        "split = int(0.8 * len(all_files))\n",
        "\n",
        "#split into training and testing\n",
        "train_files = all_files[0:split]\n",
        "test_files  = all_files[split:]\n",
        "\n",
        "train_generator = image_generator(train_files, batch_size = batch_size)\n",
        "test_generator  = image_generator(test_files, batch_size = batch_size)\n",
        "\n",
        "x, y= next(train_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCN_7SYXO03q"
      },
      "source": [
        "Displaying an example of training patch image. You may change the array index to see other examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnuNW20FeupL"
      },
      "source": [
        "plt.axis('off')\n",
        "i = 10\n",
        "img = x[i]\n",
        "msk = y[i].squeeze()\n",
        "msk = np.stack((msk,)*3, axis=-1)\n",
        "print(img.shape)\n",
        "\n",
        "plt.imshow( np.concatenate([img, msk, img*msk], axis = 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raXLOfBk1pLV"
      },
      "source": [
        "##2-2. IoU metric\n",
        "\n",
        "The intersection over union (IoU) metric is a simple metric used to evaluate the performance of a segmentation algorithm. Given two masks $x_{true}, x_{pred}$ we evaluate \n",
        "\n",
        "$$IoU = \\frac{y_{true} \\cap y_{pred}}{y_{true} \\cup y_{pred}}$$\n",
        "\n",
        "詳しくはhttps://mathwords.net/iou"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJLriYXX1oZU"
      },
      "source": [
        "def mean_iou(y_true, y_pred):\n",
        "    yt0 = y_true[:,:,:,0]\n",
        "    yp0 = K.cast(y_pred[:,:,:,0] > 0.5, 'float32')\n",
        "    inter = tf.count_nonzero(tf.logical_and(tf.equal(yt0, 1), tf.equal(yp0, 1)))\n",
        "    union = tf.count_nonzero(tf.add(yt0, yp0))\n",
        "    iou = tf.where(tf.equal(union, 0), 1., tf.cast(inter/union, 'float32'))\n",
        "    return iou"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "160g6Ex41r-2"
      },
      "source": [
        "##2-3. Model\n",
        "Choose a cell to execute for 2-3-1 Without pre-trained model or 2-3-2 With pre-trained model.\n",
        "\n",
        "FYI - an option \"decay\" is for decaying learning rate in optimizers, which sometimes useful to complex problems. See [the guide](https://keras.io/optimizers/) for details.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hONrrUbW9CM_"
      },
      "source": [
        "def unet(patch_size=patch_size, pretrained_weights=None):\n",
        "  sz = (patch_size, patch_size, 3)\n",
        "  x = Input(sz)\n",
        "  inputs = x  \n",
        "  \n",
        "  #down sampling \n",
        "  f = 8\n",
        "  layers = []\n",
        "  \n",
        "  for i in range(0, depth):\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    layers.append(x)\n",
        "    x = MaxPooling2D() (x)\n",
        "    f = f*2\n",
        "  ff2 = 64 \n",
        "  \n",
        "  #bottleneck \n",
        "  j = len(layers) - 1\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
        "  x = Concatenate(axis=3)([x, layers[j]])\n",
        "  j = j -1 \n",
        "  \n",
        "  #upsampling \n",
        "  for i in range(0, depth-1):\n",
        "    ff2 = ff2//2\n",
        "    f = f // 2 \n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "    x = Conv2DTranspose(ff2, 2, strides=(2, 2), padding='same') (x)\n",
        "    x = Concatenate(axis=3)([x, layers[j]])\n",
        "    j = j -1 \n",
        "   \n",
        "  \n",
        "  #classification \n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  x = Conv2D(f, 3, activation='relu', padding='same') (x)\n",
        "  outputs = Conv2D(1, 1, activation='sigmoid') (x)\n",
        "  \n",
        "  #model creation \n",
        "  model = Model(inputs=[inputs], outputs=[outputs])\n",
        "  if pretrained_weights is not None:\n",
        "    model.load_weights(pretrained_weights) # Loading pretrained model.\n",
        "\n",
        "  model.compile(optimizer = keras.optimizers.Adam(lr=learning_rate, decay=decay_rate), loss = 'binary_crossentropy', metrics = ['accuracy',mean_iou]) #,\n",
        "  #learning_rate\n",
        "  return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7QY8rgO1zUU"
      },
      "source": [
        "##2-4. Callbacks\n",
        "\n",
        "Simple functions to save the model at each epoch and show some predictions. See [Keras user guide](https://keras.io/callbacks/#modelcheckpoint) for file naming of models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfqXmNuc9lWZ"
      },
      "source": [
        "def build_callbacks():\n",
        "    checkpointer = ModelCheckpoint(filepath = log_d + '/weights.' + timestamp + '.{epoch:04d}-{val_loss:.4f}.hdf5', verbose=0, save_weights_only=True, save_best_only=True)\n",
        "    callbacks = [checkpointer,PlotLearning()]\n",
        "    return callbacks\n",
        "\n",
        "# inheritance for training process plot \n",
        "class PlotLearning(keras.callbacks.Callback):\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0\n",
        "        self.x = []\n",
        "        self.losses = []\n",
        "        self.val_losses = []\n",
        "        self.acc = []\n",
        "        self.val_acc = []\n",
        "        #self.fig = plt.figure()\n",
        "        self.logs = []\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        #choose a random test image and preprocess\n",
        "        path = np.random.choice(test_files)\n",
        "        raw = Image.open(patch_img + f'/{path}')\n",
        "        raw = np.array(raw)/255.\n",
        "        #print(raw.shape)\n",
        "        raw = raw[:,:,0:3]\n",
        "        \n",
        "        #predict the mask \n",
        "        pred = model.predict(np.expand_dims(raw, 0))\n",
        "        \n",
        "        #mask post-processing \n",
        "        msk  = pred.squeeze()\n",
        "        msk = np.stack((msk,)*3, axis=-1)\n",
        "        msk[msk >= 0.5] = 1 \n",
        "        msk[msk < 0.5] = 0 \n",
        "        \n",
        "        #show the mask and the segmented image \n",
        "        combined = np.concatenate([raw, msk, raw* msk], axis = 1)\n",
        "        plt.axis('off')\n",
        "        plt.imshow(combined)\n",
        "        plt.show()\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU6SPuVY8Mdc"
      },
      "source": [
        "##2-5. Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MXGinNg9Wjj"
      },
      "source": [
        "model = unet(patch_size, pretrained_weights)\n",
        "train_steps = len(train_files)/batch_size\n",
        "test_steps = len(test_files)/batch_size\n",
        "model_history = model.fit_generator(train_generator, \n",
        "                    epochs = n_epoch, \n",
        "                    steps_per_epoch = train_steps,\n",
        "                    validation_data = test_generator, \n",
        "                    validation_steps = test_steps,\n",
        "                    callbacks = build_callbacks(), \n",
        "                    verbose = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_mu0SAbt40Q"
      },
      "source": [
        "loss = model_history.history['loss']\n",
        "val_loss = model_history.history['val_loss']\n",
        "\n",
        "epochs = range(n_epoch)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0.0, 1.0])\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0O5zCKBr8OZ1"
      },
      "source": [
        "##2-6. Testing - Applying the model to the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up5RV1DXUgvd"
      },
      "source": [
        "%%script env RASDIR=\"$training_img\" MASKDIR=\"$training_ann_ras\" PATRASDIR=\"$patch_img\" PATMASKDIR=\"$patch_ann\" PATCH_SIZE=\"$patch_size\" N_PATCH=\"$n_patch\" bash\n",
        "\n",
        "IFS='\n",
        "'\n",
        "rm -rf $PATMASKDIR $PATRASDIR\n",
        "mkdir -p $PATMASKDIR $PATRASDIR\n",
        "echo $PATMASKDIR $PATRASDIR\n",
        "\n",
        "for TIF in $(find \"$RASDIR\" -type f | grep -e \"\\.png$\" -e \"\\.tif$\"); do\n",
        "  IFS=' '\n",
        "  JSON=$(gdalinfo -json \"$TIF\")\n",
        "  SIZE=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['size'])\" | tr -d [],))\n",
        "  X_SIZE=${SIZE[0]}\n",
        "  Y_SIZE=${SIZE[1]}\n",
        "  #N_PATCH_X=$(expr $(expr $X_SIZE / $PATCH_SIZE) + 1)\n",
        "  #N_PATCH_Y=$(expr $(expr $Y_SIZE / $PATCH_SIZE) + 1)\n",
        "  N_PATCH_X=$(perl -e \"use POSIX qw(floor ceil); print ceil($X_SIZE / $PATCH_SIZE)\")\n",
        "  N_PATCH_Y=$(perl -e \"use POSIX qw(floor ceil); print ceil($Y_SIZE / $PATCH_SIZE)\")  \n",
        "  X_SIZE_EXTEND=$(expr $N_PATCH_X \\* $PATCH_SIZE)\n",
        "  Y_SIZE_EXTEND=$(expr $N_PATCH_Y \\* $PATCH_SIZE)\n",
        "  gdal_translate -srcwin 0 0 $X_SIZE_EXTEND $Y_SIZE_EXTEND \"$TIF\" \"$PATRASDIR/$(basename \"$TIF\")-i.tif\"\n",
        "  \n",
        "done\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aS4T87LaR_5V"
      },
      "source": [
        "\n",
        "log_d = root_folder + '/logs/2020-02-12 01:07:10.756811'\n",
        "best_weight = log_d + '/' + sorted(os.listdir(log_d), reverse=True)[0] \n",
        "\n",
        "im = Image.open(patch_img + os.listdir(patch_img)[0])\n",
        "out_patch_size = im.size[0]\n",
        "print(patch_img)\n",
        "\n",
        "out_model = unet(out_patch_size, best_weight)\n",
        "\n",
        "try:\n",
        "    shutil.rmtree(patch_pred)\n",
        "except:\n",
        "    pass\n",
        "\n",
        "os.makedirs(patch_pred, exist_ok=True)\n",
        "\n",
        "all_files = os.listdir(patch_img)\n",
        "for f in all_files:\n",
        "  if f.endswith(\".tif\"):\n",
        "    raw = Image.open(patch_img + '/' + f)\n",
        "    raw = np.array(raw)\n",
        "    raw = np.array(raw)/255.\n",
        "    raw = raw[:,:,0:3]\n",
        "\n",
        "    pred = out_model.predict(np.expand_dims(raw, 0))\n",
        "    pred[pred >= 0.5] = 1\n",
        "    pred[pred < 0.5] = 0\n",
        "    pred = pred.astype(np.uint8).reshape([out_patch_size,out_patch_size])\n",
        "    im = Image.fromarray(pred)\n",
        "    im.save(patch_pred + '/' + f + '.pred.tif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xaq1HmoIvTU9"
      },
      "source": [
        "%%script env RASDIR=\"$training_img\" MASKDIR=\"$training_ann_ras\" PATRASDIR=\"$patch_img\" PATMASKDIR=\"$patch_ann\" PATCH_SIZE=\"$patch_size\" PATPRED=\"$patch_pred\" TEST_RESULTS=\"$test_results\" bash\n",
        "IFS='\n",
        "'\n",
        "mkdir -p $TEST_RESULTS\n",
        "\n",
        "for IMG_EXTENDED in $(find $PATRASDIR -type f -regex \".*tif$\"); do\n",
        "    IFS=' '\n",
        "    JSON=$(gdalinfo -json \"$IMG_EXTENDED\")\n",
        "    SIZE=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['size'])\" | tr -d [],))\n",
        "    X_SIZE=${SIZE[0]}\n",
        "    Y_SIZE=${SIZE[1]}\n",
        "\n",
        "    PRED_TIF=$PATPRED/$(basename \"$IMG_EXTENDED\").pred.tif\n",
        "\n",
        "    upperLeft=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['cornerCoordinates']['upperLeft'])\" | tr -d [],))\n",
        "    lowerRight=($(echo $JSON | python3 -c \"import sys, json; print(json.load(sys.stdin)['cornerCoordinates']['lowerRight'])\" | tr -d [],))\n",
        "\n",
        "    gdal_translate -q -ot Byte -a_srs EPSG:3857 -a_ullr ${upperLeft[0]} ${upperLeft[1]} ${lowerRight[0]} ${lowerRight[1]} \"$PRED_TIF\" \"$TEST_RESULTS/$(basename \"$PRED_TIF\").g.tif\"\n",
        "\n",
        "done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VG7y3dqNkgE"
      },
      "source": [
        "# トラブルシューティング\n",
        "\n",
        "1. val_mean_iouがずっとゼロのまま <-- learning rateを小さくしてみてください。\n",
        "\n",
        "2. モデルのlossが小さくなる（mean_iouが大きくなる）一方で、validation lossが大きくなる（val_mean_iouが小さくなる）つづけている <-- Overfitting 過学習が起こっています。n_patchでサンプル数を増やしてみましょう。\n",
        "\n",
        "3. 計算に時間がかかる <-- batch_size をメモリ容量が許すまで目一杯大きくしましょう。\n",
        "\n",
        "4. どうしても精度が上がらない <-- トレーニングデータの手入力精度に問題があるかもしれません。トレーニングデータの手直しをしてみましょう。分解能 spatial resolution をそろえるのも効果的です。"
      ]
    }
  ]
}